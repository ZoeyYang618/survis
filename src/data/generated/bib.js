const generatedBibEntries = {
    "7989236": {
        "author": "Wang, Sen and Clark, Ronald and Wen, Hongkai and Trigoni, Niki",
        "booktitle": "2017 IEEE International Conference on Robotics and Automation (ICRA)",
        "doi": "10.1109/ICRA.2017.7989236",
        "number": "",
        "pages": "2043-2050",
        "title": "DeepVO: Towards end-to-end visual odometry with deep Recurrent Convolutional Neural Networks",
        "type": "INPROCEEDINGS",
        "volume": "",
        "year": "2017"
    },
    "8099747": {
        "abstract": "Generation of 3D data by deep neural networks has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collections of images; however, these representations obscure the natural invariance of 3D shapes under geometric transformations, and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output - point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthodox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of-the-art methods on single image based 3d reconstruction benchmarks; but it also shows strong performance for 3D shape completion and promising ability in making multiple plausible predictions.",
        "address": "Los Alamitos, CA, USA",
        "author": "H. Fan and H. Su and L. Guibas",
        "booktitle": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
        "doi": "10.1109/CVPR.2017.264",
        "issn": "1063-6919",
        "keywords": "three-dimensional displays;shape;image reconstruction;geometry;neural networks;two dimensional displays;training",
        "month": "jul",
        "pages": "2463-2471",
        "publisher": "IEEE Computer Society",
        "title": "A Point Set Generation Network for 3D Object Reconstruction from a Single Image",
        "type": "INPROCEEDINGS",
        "url": "https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.264",
        "volume": "",
        "year": "2017"
    },
    "bloesch2018codeslam": {
        "author": "Bloesch, Michael and Czarnowski, Jan and Clark, Ronald and Leutenegger, Stefan and Davison, Andrew J",
        "booktitle": "Proceedings of the IEEE conference on computer vision and pattern recognition",
        "pages": "2560--2568",
        "title": "CodeSLAM\u2014learning a compact, optimisable representation for dense visual SLAM",
        "type": "inproceedings",
        "year": "2018"
    },
    "chen2019pointbased": {
        "archiveprefix": "arXiv",
        "author": "Rui Chen and Songfang Han and Jing Xu and Hao Su",
        "eprint": "1908.04422",
        "primaryclass": "cs.CV",
        "title": "Point-Based Multi-View Stereo Network",
        "type": "misc",
        "year": "2019"
    },
    "choy20163d": {
        "author": "Choy, Christopher B and Xu, Danfei and Gwak, JunYoung and Chen, Kevin and Savarese, Silvio",
        "booktitle": "Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14",
        "organization": "Springer",
        "pages": "628--644",
        "title": "3d-r2n2: A unified approach for single and multi-view 3d object reconstruction",
        "type": "inproceedings",
        "year": "2016"
    },
    "eigen2014depth": {
        "archiveprefix": "arXiv",
        "author": "David Eigen and Christian Puhrsch and Rob Fergus",
        "eprint": "1406.2283",
        "primaryclass": "cs.CV",
        "title": "Depth Map Prediction from a Single Image using a Multi-Scale Deep Network",
        "type": "misc",
        "year": "2014"
    },
    "mildenhall2020nerf": {
        "archiveprefix": "arXiv",
        "author": "Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng",
        "eprint": "2003.08934",
        "primaryclass": "cs.CV",
        "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
        "type": "misc",
        "year": "2020"
    },
    "tang2019banet": {
        "archiveprefix": "arXiv",
        "author": "Chengzhou Tang and Ping Tan",
        "eprint": "1806.04807",
        "primaryclass": "cs.CV",
        "title": "BA-Net: Dense Bundle Adjustment Network",
        "type": "misc",
        "year": "2019"
    },
    "wang2018pixel2mesh": {
        "archiveprefix": "arXiv",
        "author": "Nanyang Wang and Yinda Zhang and Zhuwen Li and Yanwei Fu and Wei Liu and Yu-Gang Jiang",
        "eprint": "1804.01654",
        "primaryclass": "cs.CV",
        "title": "Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images",
        "type": "misc",
        "year": "2018"
    },
    "yu2021pixelnerf": {
        "archiveprefix": "arXiv",
        "author": "Alex Yu and Vickie Ye and Matthew Tancik and Angjoo Kanazawa",
        "eprint": "2012.02190",
        "primaryclass": "cs.CV",
        "title": "pixelNeRF: Neural Radiance Fields from One or Few Images",
        "type": "misc",
        "year": "2021"
    }
};