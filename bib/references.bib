@INPROCEEDINGS{7989236,
  author={Wang, Sen and Clark, Ronald and Wen, Hongkai and Trigoni, Niki},
  booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={DeepVO: Towards end-to-end visual odometry with deep Recurrent Convolutional Neural Networks}, 
  year={2017},
  volume={},
  number={},
  pages={2043-2050},
  doi={10.1109/ICRA.2017.7989236}}


@misc{tang2019banet,
      title={BA-Net: Dense Bundle Adjustment Network}, 
      author={Chengzhou Tang and Ping Tan},
      year={2019},
      eprint={1806.04807},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@inproceedings{bloesch2018codeslam,
  title={CodeSLAMâ€”learning a compact, optimisable representation for dense visual SLAM},
  author={Bloesch, Michael and Czarnowski, Jan and Clark, Ronald and Leutenegger, Stefan and Davison, Andrew J},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2560--2568},
  year={2018}
}


@misc{eigen2014depth,
      title={Depth Map Prediction from a Single Image using a Multi-Scale Deep Network}, 
      author={David Eigen and Christian Puhrsch and Rob Fergus},
      year={2014},
      eprint={1406.2283},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@inproceedings{choy20163d,
  title={3d-r2n2: A unified approach for single and multi-view 3d object reconstruction},
  author={Choy, Christopher B and Xu, Danfei and Gwak, JunYoung and Chen, Kevin and Savarese, Silvio},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14},
  pages={628--644},
  year={2016},
  organization={Springer}
}


@INPROCEEDINGS {8099747,
author = {H. Fan and H. Su and L. Guibas},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {A Point Set Generation Network for 3D Object Reconstruction from a Single Image},
year = {2017},
volume = {},
issn = {1063-6919},
pages = {2463-2471},
abstract = {Generation of 3D data by deep neural networks has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collections of images; however, these representations obscure the natural invariance of 3D shapes under geometric transformations, and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output - point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthodox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of-the-art methods on single image based 3d reconstruction benchmarks; but it also shows strong performance for 3D shape completion and promising ability in making multiple plausible predictions.},
keywords = {three-dimensional displays;shape;image reconstruction;geometry;neural networks;two dimensional displays;training},
doi = {10.1109/CVPR.2017.264},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.264},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jul}
}


@misc{chen2019pointbased,
      title={Point-Based Multi-View Stereo Network}, 
      author={Rui Chen and Songfang Han and Jing Xu and Hao Su},
      year={2019},
      eprint={1908.04422},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@misc{wang2018pixel2mesh,
      title={Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images}, 
      author={Nanyang Wang and Yinda Zhang and Zhuwen Li and Yanwei Fu and Wei Liu and Yu-Gang Jiang},
      year={2018},
      eprint={1804.01654},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@misc{mildenhall2020nerf,
      title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis}, 
      author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
      year={2020},
      eprint={2003.08934},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{yu2021pixelnerf,
      title={pixelNeRF: Neural Radiance Fields from One or Few Images}, 
      author={Alex Yu and Vickie Ye and Matthew Tancik and Angjoo Kanazawa},
      year={2021},
      eprint={2012.02190},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}






